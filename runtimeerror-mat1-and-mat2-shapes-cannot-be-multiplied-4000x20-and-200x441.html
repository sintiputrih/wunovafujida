<!doctype html><html lang=en><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=referrer content="no-referrer"><meta name=description content="The architecture of the decoder of my variational autoencoder is given in the snippet below running my code to reconstruct the images: I got this error message: Update I changed my code slightly to this"><meta name=robots content="index,follow,noarchive"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><title>RuntimeError: mat1 and mat2 shapes cannot be multiplied (4000x20 and 200x441)</title><link rel=canonical href=./runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-4000x20-and-200x441.html><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a href=./index.html>VoxVibe</a></h1><ul><li><a href=./index.xml>Rss</a></li><li><a href=./sitemap.xml>Sitemap</a></li></ul></section><section id=content><h1>RuntimeError: mat1 and mat2 shapes cannot be multiplied (4000x20 and 200x441)</h1><div id=sub-header>March 2024 Â· 4 minute read</div><div class=entry-content><img src=https://cdn.statically.io/img/cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><p>The architecture of the decoder of my variational autoencoder is given in the snippet below</p><pre><code>class ConvolutionalVAE(nn.Module): def __init__(self, nchannel, base_channels, z_dim, hidden_dim, device, img_width, batch_size): super(ConvolutionalVAE, self).__init__() self.nchannel = nchannel self.base_channels = base_channels self.z_dim = z_dim self.hidden_dim = hidden_dim self.device = device self.img_width = img_width self.batch_size = batch_size self.enc_kernel = 4 self.enc_stride = 2 self._to_linear = None ######################## # ENCODER-CONVOLUTION LAYERS self.conv0 = nn.Conv2d(nchannel, base_channels, self.enc_kernel, stride=self.enc_stride) self.bn2d_0 = nn.BatchNorm2d(self.base_channels) self.LeakyReLU_0 = nn.LeakyReLU(0.2) out_width = np.floor((self.img_width - self.enc_kernel) / self.enc_stride + 1) self.conv1 = nn.Conv2d(base_channels, base_channels*2, self.enc_kernel, stride=self.enc_stride) self.bn2d_1 = nn.BatchNorm2d(base_channels*2) self.LeakyReLU_1 = nn.LeakyReLU(0.2) out_width = np.floor((out_width - self.enc_kernel) / self.enc_stride + 1) self.conv2 = nn.Conv2d(base_channels*2, base_channels*4, self.enc_kernel, stride=self.enc_stride) self.bn2d_2 = nn.BatchNorm2d(base_channels*4) self.LeakyReLU_2 = nn.LeakyReLU(0.2) out_width = np.floor((out_width - self.enc_kernel) / self.enc_stride + 1) self.conv3 = nn.Conv2d(base_channels*4, base_channels*8, self.enc_kernel, stride=self.enc_stride) self.bn2d_3 = nn.BatchNorm2d(base_channels*8) self.LeakyReLU_3 = nn.LeakyReLU(0.2) out_width = int(np.floor((out_width - self.enc_kernel) / self.enc_stride + 1)) ######################## #ENCODER-USING FULLY CONNECTED LAYERS #THE LATENT SPACE (Z) self.flatten = nn.Flatten() self.fc0 = nn.Linear((out_width**2) * base_channels * 8, base_channels*8*4*4, bias=False) self.bn1d = nn.BatchNorm1d(base_channels*8*4*4) self.fc1 = nn.Linear(base_channels*8*4*4, hidden_dim, bias=False) self.bn1d_1 = nn.BatchNorm1d(hidden_dim) # mean of z self.fc2 = nn.Linear(hidden_dim, z_dim, bias=False) self.bn1d_2 = nn.BatchNorm1d(z_dim) # variance of z self.fc3 = nn.Linear(hidden_dim, z_dim, bias=False) self.bn1d_3 = nn.BatchNorm1d(z_dim) ######################## # DECODER: # P(X|Z) conv2d_transpose_kernels, conv2d_transpose_input_width = self.determine_decoder_params(self.z_dim, self.img_width) self.conv2d_transpose_input_width = conv2d_transpose_input_width self.px_z_fc_0 = nn.Linear(self.z_dim, conv2d_transpose_input_width ** 2) self.px_z_bn1d_0 = nn.BatchNorm1d(conv2d_transpose_input_width ** 2) self.px_z_fc_1 = nn.Linear(conv2d_transpose_input_width ** 2, conv2d_transpose_input_width ** 2) #self.unflatten = nn.Unflatten(1, (1, conv2d_transpose_input_width, conv2d_transpose_input_width)) self.conv2d_transpose_input_width = conv2d_transpose_input_width self.px_z_conv_transpose2d = nn.ModuleList() self.px_z_bn2d = nn.ModuleList() self.n_conv2d_transpose = len(conv2d_transpose_kernels) self.px_z_conv_transpose2d.append(nn.ConvTranspose2d(1, self.base_channels * (self.n_conv2d_transpose - 1), kernel_size=conv2d_transpose_kernels[0], stride=2)) self.px_z_bn2d.append(nn.BatchNorm2d(self.base_channels * (self.n_conv2d_transpose - 1))) self.px_z_LeakyReLU = nn.ModuleList() self.px_z_LeakyReLU.append(nn.LeakyReLU(0.2)) for i in range(1, self.n_conv2d_transpose - 1): self.px_z_conv_transpose2d.append(nn.ConvTranspose2d(self.base_channels * (self.n_conv2d_transpose - i), self.base_channels*(self.n_conv2d_transpose - i - 1), kernel_size=conv2d_transpose_kernels[i], stride=2)) self.px_z_bn2d.append(nn.BatchNorm2d(self.base_channels * (self.n_conv2d_transpose - i - 1))) self.px_z_LeakyReLU.append(nn.LeakyReLU(0.2)) self.px_z_conv_transpose2d.append(nn.ConvTranspose2d(self.base_channels, self.nchannel, kernel_size=conv2d_transpose_kernels[-1], stride=2)) self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') self.to(device=self.device) def decoder(self, z_input): #Generate X: P(X|Z) h = F.relu(self.px_z_bn1d_0(self.px_z_fc_0(z_input))) flattened_h = self.px_z_fc_1(h) h = flattened_h.view(flattened_h.size()[0], 1, self.conv2d_transpose_input_width, self.conv2d_transpose_input_width) for i in range(self.n_conv2d_transpose - 1): h = self.px_z_LeakyReLU[i](self.px_z_bn2d[i](self.px_z_conv_transpose2d[i](h))) x_recons_mean_flat = torch.sigmoid(self.px_z_conv_transpose2d[self.n_conv2d_transpose - 1](h)) return x_recons_mean_flat </code></pre><p>running my code to reconstruct the images:</p><pre><code>all_z = [] for d in range(self.z_dim): temp_z = torch.cat( [self.z_sample_list[k][:, d].unsqueeze(1) for k in range(self.K)], dim=1) print(f'size of each z component dimension: {temp_z.size()}') all_z.append(torch.mm(temp_z.transpose(1, 0), components).unsqueeze(1)) out = torch.cat( all_z,1) x_samples = self.decoder(out) </code></pre><p>I got this error message:</p><pre><code>size of z dimension: 200 size of each z component dimension: torch.Size([50, 20]) size of all z component dimension: torch.Size([20, 200, 20]) x_samples = self.decoder(out) File "VAE.py", line 241, in decoder h = F.relu(self.px_z_bn1d_0(self.px_z_fc_0(z_input))) File "/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl return forward_call(*input, **kwargs) File "/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in forward return F.linear(input, self.weight, self.bias) File "/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1847, in linear return torch._C._nn.linear(input, weight, bias) RuntimeError: mat1 and mat2 shapes cannot be multiplied (4000x20 and 200x441) </code></pre><p><strong>Update</strong> I changed my code slightly to this</p><pre><code>all_z = [] for d in range(self.z_dim): temp_z = torch.cat( [self.z_sample_list[k][:, d].unsqueeze(1) for k in range(self.K)], dim=1) all_z.append(torch.mm(temp_z.transpose(1, 0), components).unsqueeze(1)) out = torch.cat( all_z,1) print(f'size of all z component dimension: {out.size()}') out = F.pad(input=out, pad=(1, 0, 0,0, 0, 1), mode='constant', value=0) print(f'new size of all z component dimension after padding: {out.size()}') out = rearrange(out, 'd0 d1 d2 -&gt; d1 (d0 d2)') x_samples = self.decoder(out) </code></pre><p>Now the new error is</p><pre><code>x_samples = self.decoder(out) File "VAE.py", line 243, in decoder h = F.relu(self.px_z_bn1d_0(self.px_z_fc_0(z_input))) File "/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl return forward_call(*input, **kwargs) File "/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 96, in forward return F.linear(input, self.weight, self.bias) File "/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py", line 1847, in linear return torch._C._nn.linear(input, weight, bias) RuntimeError: mat1 and mat2 shapes cannot be multiplied (200x441 and 200x441) </code></pre><p>Any suggestion to fix this error?</p><span class=d-none itemprop=commentCount>6</span><h2 class=mb0 data-answercount=1>1 Answer</h2><p>Matrix multiplication requires the 2 inner dimensions to be the same. You are getting the error: <code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (200x441 and 200x441)</code> because your inner dimensions don't line up.</p><p>for example:</p><pre><code>shape(200, 441) * shape(441, 200) # works shape(441, 200) * shape(200, 441) # works shape(200, 441) * shape(200, 441) # doesn't work, this is why you are getting your error # in general shape(x, y) * shape(y, z) # works </code></pre><p>To make the inner dimensions match, just take the transpose of one or the other:</p><pre><code>shape(200, 441) * shape(200, 441).T # works # or shape(200, 441).T * shape(200, 441) # works # since the transpose works by swapping the dimensions: shape(200, 441).T = shape(441, 200) </code></pre><span class=d-none itemprop=commentCount></span><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmirpJawrLvVnqmfpJ%2Bse6S7zGiorp2jqbawutJobmprZGqBc4GOq6ynrJmisqa%2B0aipZqWRqX5urc2dZKaZpGd6tLTAqZysZZOWu6%2B702aZnmWdqrm1tc%2BloJ6cXWl9cXzXa2dmmZ6ZenN8j7FrbWk%3D</p></div><div id=links><a href=./lonnie-loren-kocontes-now.html>&#171;&nbsp;Where Is Lonnie Loren Kocontes Now? Tracing the Steps of the Notorious Figure</a>
<a href=./josh-hawley-net-worth.html>Josh Hawley Net Worth - 247 News Around The World&nbsp;&#187;</a></div></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>